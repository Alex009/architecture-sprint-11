# Проблемные места текущей архитектуры

## Долгое построение отчетов для аналитики (часы)

Бизнес проблема:
> Сейчас нужную отчётность сложно построить за разумное время: очень много данных (сотни терабайт) и вариантов их использования. Это порождает большое количество трансформаций, что в свою очередь замедляет time-to-market и приводит к снижению производительности. В итоге сложные отчёты можно ждать часами.

Причины:
1. Текущий Data Warehouse фактически является системой с операционной логикой клиник (в бд бизнеслогика, а к ней подключен Power Builder с интерфейсом для операторов) и в тоже время является источником данных для бизнес аналитики. 
    - Это приводит к:
        - для аналитики доступны данные, которые не должны быть доступны (медицинские карты, истории болезней, результаты исследований). 
            - Однако эти данные нужны для ИИ-сервисов, судя по описанию "ИИ-сервисы для работы с медицинскими данными в виде отдельной компании"
        - аналитические отчеты требуют много времени для построения (можно ждать часами) из-за большого количества трансформаций в той же базе данных, что и операционная логика
        - скорее всего в БД есть множество данных нужных конкретно для построения отчетов, а не для работы операционной логики клиник
        - нагрузка создаваемая от построения аналитических отчетов влияет и на работу клиник, потому что это фактически одна база данных
        - при попытке добавить новые бизнес-подразделения в аналитические отчеты потребуется вносить изменения в БД системы клиник, хотя данные не будут относиться к клиникам совсем
    - Предлагаемое решение:
        - Текущий Microsoft SQL Server, который именуется Data Warehouse, оставить как операционную логику для медицинского подразделения
        - Для аналитических отчетов и данных для обучения ИИ добавить Data Lakehouse, настроить регулярную выгрузку данных с систем-источников в него. Например на базе Minio + Apache Iceberg.
        - Переключить Power BI на чтение данных из Lakehouse, через прослойку в виде Dremio. Это позволит постепенно переключать источник данных отчетов Power BI с текущего Data Warehouse на новый Lakehouse
2. Microsoft SQL-сервер 2008 года не оптимизирован под те объемы, которые уже набрались, а также имеет скудные возможности к горизонтальному масштабированию, что также накладывает значительные ограничения на производительность.
    - Предлагаемое решение - связано с пунктом выше. Разделить операционную логику и аналитическую, за счет чего скорее всего удастся избавиться от множества данных в MSSQL, так как данные окажутся нужны только для построения отчетов и теперь будут доступны в Lakehouse.

## Поиск данных стал проблемой - есть запрос на витрину данных

Бизнес задача:
> Сделать удобную «витрину данных». Это портал самообслуживания, который легко масштабировать вне зависимости от количества бизнесов и их бизнес-целей. По задумке, сотрудник может получить здесь отчёт по любым срезам, которые выберет, если это позволяет его уровень доступа. Ещё нужно добавить возможность конструировать отчёты самостоятельно. Обратите внимание: в витрину данных не нужно добавлять медицинские карты, истории болезней и результаты медицинских исследований. Эти данные компания не будет использовать для аналитики.

Что можно сделать:  
Развернуть DataHub, который будет сканировать все источники данных (и бд каждого из подразделений, и dremio, и интеграционную шину Camel). 

С его помощью сделать:
1. разметить владельцев данных, чтобы аналитики могли понимать к кому обращаться по вопросам определенных данных
2. протегировать данные для контроля над конфиденциальностью
3. описать информацию как использовать данные, задокументировать

## Невозможно развивать бизнес-направления независимо друг от друга

Бизнес проблема:
> Бизнес хочет развивать все направления независимо, но в то же время иметь единую картину по бизнес-показателям. 

Однако с текущей архитектурой развивать направления независимо - невозможно. Текущая бизнес-аналитика опирается на данные в Data Warehouse, который в тоже время является основной базой для бизнес-направления клиник. А значит, каждое изменение желаемое подразделениями финтех/фарма/медоборудования будет упираться в необходимость согласования изменения в БД клиник и команда клиник будет всегда узким местом.

Что можно сделать:
1. Для начала выделить бизнес-домены (в соответствии с структурой компании например):
    - Клиники
    - Финтех
    - Фарма
    - Медицинское оборудование
    - ML
    - Management
2. Когда бизнес-домены определены - разделить их инфраструктуру и предоставить понятный интерфейс для обмена данными. Команда каждого из доменов должна владеть своими данными, вносить нужные им изменения и иметь возможность взаимодействовать с другими доменами через стандартизированный интерфейс взаимодействия.
    - Например оформить документацию по работе с интеграционной шиной Apache Camel, задать стандарты по реализации типовых вещей - передачу данных в Lakehouse и получение данных от единого источника правды.
3. На уровне каталога данных (в DataHub) разметить ответственность за разные данные, закрепить по бизнес-доменам

## Есть утечки конфиденциальных данных в аналитику

Часть конфиденциальных данных доступна в аналитике, так как сейчас Data Warehouse весь подключен к Power BI. В DWH содержится и медицинская информация, которая относится к конфиденциальной.

Также нет гарантий что для ML обучения используется анонимизированная медицинская информация, хотя мы точно знаем что ML обрабатывает именно медицинские данные.

Предлагаемое решение - зафиксировать стандарт работы с данными, определить что данные должны записываться в Lakehouse уже с обработанными через анонимизацию/маскирование в Apache Camel пайплайнах. С помощью DataHub контролировать что данные попадаемые в Lakehouse не содержат ничего конфиденциального, доступного для аналитики.

## Ограниченное масштабирование DWH

Текущий DWH работает на базе Microsoft SQL-сервера 2008 года, который как выше упоминалось плохо справляется с набранными объемами данных. Из-за отсутствия механизмов горизонтального масштабирования единственный путь улучшать производительность - докупать более сильное железо, но это не так эффективно на таких объемах данных, которые уже есть.

Следует рассмотреть обновление до более новых версий, с поддержкой горизонтального масштабирования (и включения данной функции).

## Трансформация данных для отчетов происходит налету

Судя по описанию "очень много данных (сотни терабайт) и вариантов их использования. Это порождает большое количество трансформаций" при построении отчетности используется множество трансформаций, которые работают "налету" - то есть в момент попытки построения отчета. Это создает нагрузку, заставляя для каждой попытки построения отчета выполнять массивную работу не только по получению данных, но и по преобразованию в нужное представление.

Исправить это можно усилив пайплайны Apache Camel, либо перейдя на Apache Airflow. В пайплайнах Camel / DAG'ах Airflow помимо процедуры передачи данных в data lakehouse должно быть реализовано и подготовка данных в вид, подходящий для последующего построения отчетов (например агрегации, преобразования).

# Приоритезация проблем

По методологии Moscow:

## Must

1. Долгое построение отчетов для аналитики (часы)
    - главная болевая точка сейчас, отчеты строятся часами.
2. Есть утечки конфиденциальных данных в аналитику
    - если на этапе работы с аналитикой конфиденциальные медицинские данные клиентов будут как-то использованы, то компания может понести серьезные убытки.

## Should

1. Невозможно развивать бизнес-направления независимо друг от друга
    - уже присоединен финтех и будут подключены еще несколько компаний, мы должны обеспечить их независимое развитие, иначе развитие будет остановлено из-за узкого места в бизнесе клиник
2. Поиск данных стал проблемой - есть запрос на витрину данных
    - бизнес явно сформулировал проблему поиска данных и запрос на витрину, это нужно сделать.

## Could

1. Ограниченное масштабирование DWH
    - в случае развития Data Lakehouse проблема ограниченного масштабирования DWH станет не такой острой - текущий DWH по сути перестанет быть хранилищем аналитической информации, она уйдет в lake, а здесь останется только оперативная логика, что позволит не торопиться с решением проблемы масштабирования, но сделать это стоит.
2. Трансформация данных для отчетов происходит налету
    - с одной стороны расчет отчетов налету может иметь прямое влияние на скорость построения отчетов, а с другой - добавление предварительной трансформации это значительные ресурсы разработки, что в начале может и подождать, в сравнении с другими пунктами.

## Won't

отсутствуют.

# Видение целевого результата

1. текущий Data Warehouse оставить только под задачу операционной логики подразделения клиник. Там уже есть бизнес-логика для работы клиник и интерфейсы через Power Builder и нет смысла делать большой переезд на другие технологии (например свой java backend, другая СУБД и отдельный web клиент для пользователей), еще и переобучать людей новой системе, исправлять баги новой разработки. Эта миграция не имеет смысла, ведь не решит никак проблему текущую - долгое построение отчетов и деградация. Однако, если разделить задачи "аналитика" и "операционка" на разные системы, тогда удастся улучшить ситуацию.
2. для задач аналитики добавить Data Lakehouse. Хранить данные в объектном, хорошо масштабируемом решении. Начать с MinIO например, либо сразу взять облачное решение от Яндекс. Хранить и данные для обычной аналитики в виде отчетов (для Power BI) и данные для ML направления.
3. для работы с данными Lakehouse поднять Dremio, через который данные в Lakehouse будут поступать, а также через него же будут и читаться системами для аналитики или обмена между разными бизнес-подразделениями
4. Power BI переключить с  чтения Data Warehouse на работу через Dremio. 
5. На время миграции подключить в Dremio и сам текущий Data Warehouse (SQL Server), чтобы текущие Power BI отчеты продолжили работать, но уже через другой источник данных. При чем в процессе миграции мы сможем заменять на уровне Dremio откуда берутся данные - когда в Lakehouse уже появятся нужные нам данные, мы переключим получение с Warehouse на Lakehouse
6. Для работы с данными бизнес-аналитики смогут работать как в Power BI (для подготовки дашбордов для остальных ролей, например директоров), так и в Dremio (при исследовании)
7. Интеграционную шину Camel в целом можно оставить, только перенастроить чтобы она отправляла данные теперь в Lakehouse со всех сервисов, через Dremio. И в тоже время через Dremio брала данные для обмена между разными сервисами (например когда данные из финтеха должны попасть в клиники).
8. Чтобы не потеряться в объеме данных, который нас ждет, добавить еще DataHub, который позволит закрепить данные за доменами, видеть происхождение данных и как они переходят между хранилищами, поставить уровни конфиденциальности.
