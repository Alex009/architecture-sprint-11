# Проблемные места текущей архитектуры

## Долгое построение отчетов для аналитики (часы)

Бизнес проблема:
> Сейчас нужную отчётность сложно построить за разумное время: очень много данных (сотни терабайт) и вариантов их использования. Это порождает большое количество трансформаций, что в свою очередь замедляет time-to-market и приводит к снижению производительности. В итоге сложные отчёты можно ждать часами.

Причины:
1. Текущий Data Warehouse фактически является системой с операционной логикой клиник (в бд бизнеслогика, а к ней подключен Power Builder с интерфейсом для операторов) и в тоже время является источником данных для бизнес аналитики. 
    - Это приводит к:
        - для аналитики доступны данные, которые не должны быть доступны (медицинские карты, истории болезней, результаты исследований). 
            - Однако эти данные нужны для ИИ-сервисов, судя по описанию "ИИ-сервисы для работы с медицинскими данными в виде отдельной компании"
        - аналитические отчеты требуют много времени для построения (можно ждать часами) из-за большого количества трансформаций в той же базе данных, что и операционная логика
        - скорее всего в БД есть множество данных нужных конкретно для построения отчетов, а не для работы операционной логики клиник
        - нагрузка создаваемая от построения аналитических отчетов влияет и на работу клиник, потому что это фактически одна база данных
        - при попытке добавить новые бизнес-подразделения в аналитические отчеты потребуется вносить изменения в БД системы клиник, хотя данные не будут относиться к клиникам совсем
    - Предлагаемое решение:
        - Текущий Microsoft SQL Server, который именуется Data Warehouse, оставить как операционную логику для медицинского подразделения
        - Для аналитических отчетов и данных для обучения ИИ добавить Data Lakehouse, настроить регулярную выгрузку данных с систем-источников в него. Например на базе Minio + Apache Iceberg.
        - Переключить Power BI на чтение данных из Lakehouse, через прослойку в виде Dremio. Это позволит постепенно переключать источник данных отчетов Power BI с текущего Data Warehouse на новый Lakehouse
2. Microsoft SQL-сервер 2008 года не оптимизирован под те объемы, которые уже набрались, а также имеет скудные возможности к горизонтальному масштабированию, что также накладывает значительные ограничения на производительность.
    - Предлагаемое решение - связано с пунктом выше. Разделить операционную логику и аналитическую, за счет чего скорее всего удастся избавиться от множества данных в MSSQL, так как данные окажутся нужны только для построения отчетов и теперь будут доступны в Lakehouse.

## Поиск данных стал проблемой - есть запрос на витрину данных

Бизнес задача:
> Сделать удобную «витрину данных». Это портал самообслуживания, который легко масштабировать вне зависимости от количества бизнесов и их бизнес-целей. По задумке, сотрудник может получить здесь отчёт по любым срезам, которые выберет, если это позволяет его уровень доступа. Ещё нужно добавить возможность конструировать отчёты самостоятельно. Обратите внимание: в витрину данных не нужно добавлять медицинские карты, истории болезней и результаты медицинских исследований. Эти данные компания не будет использовать для аналитики.

Что можно сделать:  
Развернуть DataHub, который будет сканировать все источники данных (и бд каждого из подразделений, и dremio, и интеграционную шину Camel). 

С его помощью сделать:
1. разметить владельцев данных, чтобы аналитики могли понимать к кому обращаться по вопросам определенных данных
2. протегировать данные для контроля над конфиденциальностью
3. описать информацию как использовать данные, задокументировать

## Невозможно развивать бизнес-направления независимо друг от друга

Бизнес проблема:
> Бизнес хочет развивать все направления независимо, но в то же время иметь единую картину по бизнес-показателям. 

Однако с текущей архитектурой развивать направления независимо - невозможно. Текущая бизнес-аналитика опирается на данные в Data Warehouse, который в тоже время является основной базой для бизнес-направления клиник. А значит, каждое изменение желаемое подразделениями финтех/фарма/медоборудования будет упираться в необходимость согласования изменения в БД клиник и команда клиник будет всегда узким местом.

Что можно сделать:
1. Для начала выделить бизнес-домены (в соответствии с структурой компании например):
    - Клиники
    - Финтех
    - Фарма
    - Медицинское оборудование
    - ML
    - Management
2. Когда бизнес-домены определены - разделить их инфраструктуру и предоставить понятный интерфейс для обмена данными. Команда каждого из доменов должна владеть своими данными, вносить нужные им изменения и иметь возможность взаимодействовать с другими доменами через стандартизированный интерфейс взаимодействия.
    - Например оформить документацию по работе с интеграционной шиной Apache Camel, задать стандарты по реализации типовых вещей - передачу данных в Lakehouse и получение данных от единого источника правды.
3. На уровне каталога данных (в DataHub) разметить ответственность за разные данные, закрепить по бизнес-доменам

## Есть утечки конфиденциальных данных в аналитику

Часть конфиденциальных данных доступна в аналитике, так как сейчас Data Warehouse весь подключен к Power BI. В DWH содержится и медицинская информация, которая относится к конфиденциальной.

Также нет гарантий что для ML обучения используется анонимизированная медицинская информация, хотя мы точно знаем что ML обрабатывает именно медицинские данные.

Предлагаемое решение - зафиксировать стандарт работы с данными, определить что данные должны записываться в Lakehouse уже с обработанными через анонимизацию/маскирование в Apache Camel пайплайнах. С помощью DataHub контролировать что данные попадаемые в Lakehouse не содержат ничего конфиденциального, доступного для аналитики.

## Ограниченное масштабирование DWH

Текущий DWH работает на базе Microsoft SQL-сервера 2008 года, который как выше упоминалось плохо справляется с набранными объемами данных. Из-за отсутствия механизмов горизонтального масштабирования единственный путь улучшать производительность - докупать более сильное железо, но это не так эффективно на таких объемах данных, которые уже есть.

Следует рассмотреть обновление до более новых версий, с поддержкой горизонтального масштабирования (и включения данной функции).

## Трансформация данных для отчетов происходит налету

Судя по описанию "очень много данных (сотни терабайт) и вариантов их использования. Это порождает большое количество трансформаций" при построении отчетности используется множество трансформаций, которые работают "налету" - то есть в момент попытки построения отчета. Это создает нагрузку, заставляя для каждой попытки построения отчета выполнять массивную работу не только по получению данных, но и по преобразованию в нужное представление.

Исправить это можно усилив пайплайны Apache Camel, либо перейдя на Apache Airflow. В пайплайнах Camel / DAG'ах Airflow помимо процедуры передачи данных в data lakehouse должно быть реализовано и подготовка данных в вид, подходящий для последующего построения отчетов (например агрегации, преобразования).

# Приоритезация проблем


